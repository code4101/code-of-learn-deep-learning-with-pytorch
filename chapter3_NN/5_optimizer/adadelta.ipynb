{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adadelta\n",
    "Adadelta 算是 Adagrad 法的延伸，它跟 RMSProp 一样，都是为了解决 Adagrad 中学习率不断减小的问题，RMSProp 是通过移动加权平均的方式，而 Adadelta 也是一种方法，有趣的是，它并不需要学习率这个参数。\n",
    "\n",
    "## Adadelta 法\n",
    "Adadelta 跟 RMSProp 一样，先使用移动平均来计算 s\n",
    "\n",
    "$$\n",
    "s = \\rho s + (1 - \\rho) g^2\n",
    "$$\n",
    "\n",
    "这里 $\\rho$ 和 RMSProp 中的 $\\alpha$ 都是移动平均系数，g 是参数的梯度，然后我们会计算需要更新的参数的变化量\n",
    "\n",
    "$$\n",
    "g' = \\frac{\\sqrt{\\Delta \\theta + \\epsilon}}{\\sqrt{s + \\epsilon}} g\n",
    "$$\n",
    "\n",
    "$\\Delta \\theta$ 初始为 0 张量，每一步做如下的指数加权移动平均更新\n",
    "\n",
    "$$\n",
    "\\Delta \\theta = \\rho \\Delta \\theta + (1 - \\rho) g'^2\n",
    "$$\n",
    "\n",
    "最后参数更新如下\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - g'\n",
    "$$\n",
    "\n",
    "下面我们实现以下 Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adadelta(parameters, sqrs, deltas, rho):\n",
    "    eps = 1e-6\n",
    "    for param, sqr, delta in zip(parameters, sqrs, deltas):\n",
    "        sqr[:] = rho * sqr + (1 - rho) * param.grad.data ** 2\n",
    "        cur_delta = torch.sqrt(delta + eps) / torch.sqrt(sqr + eps) * param.grad.data\n",
    "        delta[:] = rho * delta + (1 - rho) * cur_delta ** 2\n",
    "        param.data = param.data - cur_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST # 导入 pytorch 内置的 mnist 数据\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
    "    x = x.reshape((-1,)) # 拉平\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "\n",
    "train_set = MNIST('./data', train=True, transform=data_tf, download=True) # 载入数据集，申明定义的数据变换\n",
    "test_set = MNIST('./data', train=False, transform=data_tf, download=True)\n",
    "\n",
    "# 定义 loss 函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "# 使用 Sequential 定义 3 层神经网络\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10),\n",
    ")\n",
    "\n",
    "# 初始化梯度平方项和 delta 项\n",
    "sqrs = []\n",
    "deltas = []\n",
    "for param in net.parameters():\n",
    "    sqrs.append(torch.zeros_like(param.data))\n",
    "    deltas.append(torch.zeros_like(param.data))\n",
    "\n",
    "# 开始训练\n",
    "losses = []\n",
    "idx = 0\n",
    "start = time.time() # 记时开始\n",
    "for e in range(5):\n",
    "    train_loss = 0\n",
    "    for im, label in train_data:\n",
    "        im = Variable(im)\n",
    "        label = Variable(label)\n",
    "        # 前向传播\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        # 反向传播\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        adadelta(net.parameters(), sqrs, deltas, 0.9) # rho 设置为 0.9\n",
    "        # 记录误差\n",
    "        train_loss += loss.data\n",
    "        if idx % 30 == 0:\n",
    "            losses.append(loss.data)\n",
    "        idx += 1\n",
    "    print('epoch: {}, Train Loss: {:.6f}'\n",
    "          .format(e, train_loss / len(train_data)))\n",
    "end = time.time() # 计时结束\n",
    "print('使用时间: {:.5f} s'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, 5, len(losses), endpoint=True)\n",
    "plt.semilogy(x_axis, losses, label='rho=0.99')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到使用 adadelta 跑 5 次能够得到更小的 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小练习：思考一下为什么 Adadelta 没有学习率这个参数，它是被什么代替了**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然 pytorch 也内置了 adadelta 的方法，非常简单，只需要调用 `torch.optim.Adadelta()` 就可以了，下面是例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train Loss: 0.356505\n",
      "epoch: 1, Train Loss: 0.158333\n",
      "epoch: 2, Train Loss: 0.120510\n",
      "epoch: 3, Train Loss: 0.100807\n",
      "epoch: 4, Train Loss: 0.084741\n",
      "使用时间: 47.90947 s\n"
     ]
    }
   ],
   "source": [
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "# 使用 Sequential 定义 3 层神经网络\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adadelta(net.parameters(), rho=0.9)\n",
    "\n",
    "# 开始训练\n",
    "start = time.time() # 记时开始\n",
    "for e in range(5):\n",
    "    train_loss = 0\n",
    "    for im, label in train_data:\n",
    "        im = Variable(im)\n",
    "        label = Variable(label)\n",
    "        # 前向传播\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 记录误差\n",
    "        train_loss += loss.data[0]\n",
    "    print('epoch: {}, Train Loss: {:.6f}'\n",
    "          .format(e, train_loss / len(train_data)))\n",
    "end = time.time() # 计时结束\n",
    "print('使用时间: {:.5f} s'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小练习：看看 pytorch 中的 adadelta，里面是有学习率这个参数，但是前面我们讲过 adadelta 不用设置学习率，看看这个学习率到底是干嘛的**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
